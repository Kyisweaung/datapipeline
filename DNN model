# ==========================
# Step 0: Install & Imports
# ==========================
%pip install tensorflow==2.18.0

import os
os.environ["KERAS_BACKEND"] = "tensorflow"

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, r2_score
from pyspark.sql.functions import col
import tensorflow as tf

from keras.models import Sequential
from keras.layers import Dense, Dropout
from keras.callbacks import EarlyStopping, ReduceLROnPlateau
from keras.regularizers import l2

import matplotlib.pyplot as plt


# ==========================
# Step 1: Load & Prepare Data
# ==========================
df_spark = spark.table("default.kpi_tmp_all_trans_liquidity_sub_base3_uu1_update5")

# Convert all decimals/strings → double (except msisdn)
for c, t in df_spark.dtypes:
    if (t.startswith("decimal") or t == "string") and c != "msisdn":
        df_spark = df_spark.withColumn(c, col(c).cast("double"))

df = df_spark.toPandas()

# Remove msisdn (identifier)
df = df.drop(columns=["msisdn"], errors="ignore")

# Remove invalid targets
df = df[df["target_bal"] >= 0]

target = "target_bal"
features = [c for c in df.columns if c != target]

X = df[features]
y = df[target]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)


# ==========================
# Step 2: Scale Features
# ==========================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)


# ==========================
# Step 3: Learning-Rate Schedule
# ==========================
lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate=1e-3,
    decay_steps=6000,
    decay_rate=0.95
)

optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)


# ==========================
# Step 4: Build Deep SELU Neural Network
# ==========================
model = Sequential([
    Dense(512, activation="selu", kernel_initializer="lecun_normal",
          kernel_regularizer=l2(1e-6), input_shape=(X_train_scaled.shape[1],)),
    Dropout(0.1),

    Dense(256, activation="selu", kernel_initializer="lecun_normal",
          kernel_regularizer=l2(1e-6)),
    Dropout(0.1),

    Dense(128, activation="selu", kernel_initializer="lecun_normal",
          kernel_regularizer=l2(1e-6)),
    Dropout(0.05),

    Dense(64, activation="selu", kernel_initializer="lecun_normal",
          kernel_regularizer=l2(1e-6)),

    Dense(32, activation="selu", kernel_initializer="lecun_normal",
          kernel_regularizer=l2(1e-6)),

    Dense(1)   # Regression output
])

model.compile(
    optimizer=optimizer,
    loss="mean_absolute_error"
)


# ==========================
# Step 5: Callbacks
# ==========================
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=25,
    restore_best_weights=True
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=8,
    min_lr=1e-6
)


# ==========================
# Step 6: Train Model
# ==========================
history = model.fit(
    X_train_scaled, y_train,
    validation_data=(X_test_scaled, y_test),
    epochs=400,
    batch_size=2048,   # reduce to 1024 if you run out of GPU RAM
    callbacks=[reduce_lr, early_stop],
    verbose=2
)


# ==========================
# Step 7: Evaluate
# ==========================
pred = model.predict(X_test_scaled).flatten()

print(f"Final DNN MAE: {mean_absolute_error(y_test, pred):,.2f}")
print(f"Final DNN R²: {r2_score(y_test, pred):.4f}")


# ==========================
# Step 8: Plot Loss Curve
# ==========================
plt.figure(figsize=(10,5))
plt.plot(history.history['loss'], label='Train MAE')
plt.plot(history.history['val_loss'], label='Validation MAE')
plt.xlabel('Epochs')
plt.ylabel('MAE')
plt.title('DNN Training Curve')
plt.legend()
plt.show()
